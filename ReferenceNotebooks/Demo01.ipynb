{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "Live_Demo.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gx0oi-G4pCLb",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Model 1 Demo: urgency detection using pretrained GloVe embeddings + an LSTM model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z8xpNV_wuFW2",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Run this to set up the demo"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "SSeG_rMHgLZC",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "56bf44b3-7c1b-4593-e770-c1df97b688f6",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "#unzip model \n",
    "!unzip /content/glove_lstm_model.zip -d glove_lstm_model"
   ],
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Archive:  /content/glove_lstm_model.zip\n",
      "   creating: glove_lstm_model/model/\n",
      "  inflating: glove_lstm_model/__MACOSX/._model  \n",
      "   creating: glove_lstm_model/model/variables/\n",
      "  inflating: glove_lstm_model/__MACOSX/model/._variables  \n",
      "  inflating: glove_lstm_model/model/saved_model.pb  \n",
      "  inflating: glove_lstm_model/__MACOSX/model/._saved_model.pb  \n",
      "   creating: glove_lstm_model/model/assets/\n",
      "  inflating: glove_lstm_model/__MACOSX/model/._assets  \n",
      "  inflating: glove_lstm_model/model/variables/variables.data-00000-of-00001  \n",
      "  inflating: glove_lstm_model/__MACOSX/model/variables/._variables.data-00000-of-00001  \n",
      "  inflating: glove_lstm_model/model/variables/variables.index  \n",
      "  inflating: glove_lstm_model/__MACOSX/model/variables/._variables.index  \n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "P8NqGmJriDxf",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "29c59bcb-5dbf-4321-c042-a7168795bc84",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "#load model\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "model = keras.models.load_model('glove_lstm_model/model')"
   ],
   "execution_count": 4,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "o4PPAIG0k1-W",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "abf4f951-820d-4622-f071-01a4e063d261",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# read the csv data in the google sheets and gather all of the sentences  \n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"Urgency Sentences - Sheet1.csv\", encoding=\"utf-8\") \n",
    "\n",
    "#import nltk and clean the corpus\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk \n",
    "\n",
    "sentences = list()\n",
    "lines = df['Sentence'].values.tolist()\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "for line in lines:   \n",
    "    tokens = word_tokenize(line)\n",
    "    # convert to lower case\n",
    "    tokens = [w.lower() for w in tokens]\n",
    "    # remove punctuation from each word    \n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    stripped = [w.translate(table) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    words = [word for word in stripped if word.isalpha()]\n",
    "    # filter out stop words    \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    sentences.append(words)\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "max_length = 100\n",
    "\n",
    "# vectorize the text samples into a 2D integer tensor\n",
    "tokenizer_obj = Tokenizer() \n",
    "tokenizer_obj.fit_on_texts(sentences)"
   ],
   "execution_count": 29,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qbri52k1uNpL",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Demo"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "D39mtcgzivM6",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "11c2ffab-5034-4cc3-a651-fda43234c743",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "test_phrase = \"Six people are in dire need of food.\" #@param {type:\"string\"}\n",
    "tokens = word_tokenize(test_phrase)\n",
    "# convert to lower case\n",
    "tokens = [w.lower() for w in tokens]\n",
    "# remove punctuation from each word    \n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "stripped = [w.translate(table) for w in tokens]\n",
    "# remove remaining tokens that are not alphabetic\n",
    "words = [word for word in stripped if word.isalpha()]\n",
    "# filter out stop words    \n",
    "stop_words = set(stopwords.words('english'))\n",
    "words = [w for w in words if not w in stop_words]\n",
    "\n",
    "sequences = tokenizer_obj.texts_to_sequences([words])\n",
    "review_pad = pad_sequences(sequences, maxlen=max_length)\n",
    "\n",
    "print('Sentence being analyzed by the model: ', test_phrase)\n",
    "print('Model confidence that this sentence conveys urgency: {}%'.format(int(model.predict(review_pad)[0][0]*100)))"
   ],
   "execution_count": 31,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Sentence being analyzed by the model:  Six people are in dire need of food.\n",
      "Model confidence that this sentence conveys urgency: 99%\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LSyznwCrsgRw",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Model 2 Demo: Sentiment Analysis using trained Word2Vec embeddings and GRU\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u0dwlfIruWQu",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Run this to set up the demo"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "AvNU9qUfrTjc",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Loads both the model and the tokenizer \n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "model_output = 'sentiment-model.h5'\n",
    "\n",
    "model = keras.models.load_model(model_output)\n",
    "tokenizer_output = 'tokenizer.pickle'\n",
    "\n",
    "with open(tokenizer_output, 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)"
   ],
   "execution_count": 17,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vUX5WcUluaPo",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Demo"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "_5qBgx_st6BJ",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "dfd1ba5c-fae8-40e7-81de-b6a9ea1433a9",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Demo of the model - user can enter in input and see the percentage value that it is a positive phrase \n",
    "import re \n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"wordnet\")\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "sentence = input(\"Enter in the phrase you would like to determine positivity/negativity for:\")\n",
    "\n",
    "# removes punctuation in the sentence\n",
    "sentence = re.sub(r'[^\\w\\s]', '', sentence)  \n",
    "# gets all of the words in the sentence \n",
    "words = word_tokenize(sentence) \n",
    "# converts all the words to lowercase\n",
    "words = [w.lower() for w in words]   \n",
    "# retrieves a set of all of the stopwords in English \n",
    "stop_words = set(stopwords.words('english')) \n",
    "# removes the stopwords in the sentence\n",
    "words = [w for w in words if not w in stop_words]  \n",
    "\n",
    "cleaned_sentence = [words] \n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(cleaned_sentence)\n",
    "review_pad = pad_sequences(sequences, maxlen=100)\n",
    "\n",
    "print('Model confidence that this sentence conveys positivity: {}%'.format(int(model.predict(review_pad)[0][0]*100)))"
   ],
   "execution_count": 19,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "Enter in the phrase you would like to determine positivity/negativity for:I am so sorry\n",
      "Model confidence that this sentence conveys positivity: 9%\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JFlLkLv-shxH",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Model 3 Demo: Urgency Detection using pretrained FastText embeddings and LSTM\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U7TJQY26uXD8",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Run this to set up the demo"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "8aDGnt1ysJS8",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Loads both the model and the tokenizer \n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "model_output = 'urgency-model.h5'\n",
    "\n",
    "model = keras.models.load_model(model_output)\n",
    "\n",
    "tokenizer_output = 'urgency-tokenizer.pickle'\n",
    "\n",
    "with open(tokenizer_output, 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)"
   ],
   "execution_count": 20,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UzAsgZaFua7G",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Demo"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "AQJzCR2nsg5q",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "8394d493-9f6a-432f-a036-f2344d0445d7",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Demo of the model - user can enter in input and see the percentage value that it is a positive phrase \n",
    "import re \n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"wordnet\")\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "sentence = input(\"Enter in the phrase you would like to determine urgency for:\")\n",
    "\n",
    "# removes punctuation in the sentence\n",
    "sentence = re.sub(r'[^\\w\\s]', '', sentence)\n",
    "# gets all of the words in the sentence \n",
    "words = word_tokenize(sentence)\n",
    "# converts all the words to lowercase\n",
    "words = [w.lower() for w in words]\n",
    "# Converts each word to its lemmas form \n",
    "words = [WordNetLemmatizer().lemmatize(w) for w in words] \n",
    "# retrieves a set of all of the stopwords in English \n",
    "stop_words = set(stopwords.words('english'))\n",
    "# removes the stopwords in the sentence\n",
    "words = [w for w in words if not w in stop_words]\n",
    "\n",
    "cleaned_sentence = [words]\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(cleaned_sentence)\n",
    "review_pad = pad_sequences(sequences, maxlen=100)\n",
    "\n",
    "print('Model confidence that this sentence conveys urgency: {}%'.format(int(model.predict(review_pad)[0][0]*100)))"
   ],
   "execution_count": 24,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "Enter in the phrase you would like to determine urgency for:You will miss the train\n",
      "Model confidence that this sentence conveys urgency: 40%\n"
     ]
    }
   ]
  }
 ]
}